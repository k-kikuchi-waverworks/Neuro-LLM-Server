# main.py
import base64
import platform
import os
import sys
import importlib.metadata
from typing import Union, List
from pydantic import BaseModel
from fastapi import FastAPI, Response
from sse_starlette.sse import EventSourceResponse
import torch
from PIL import Image
from io import BytesIO
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig

# ãƒ€ãƒŸãƒ¼ã®bitsandbytesãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦importlib.metadata.versionã®ãƒã‚§ãƒƒã‚¯ã‚’å›é¿
class DummyBitsAndBytes:
    __version__ = "0.0.0"  # é©å½“ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¨­å®š
sys.modules['bitsandbytes'] = DummyBitsAndBytes()

# M5 Macã®å ´åˆã€é‡å­åŒ–ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
is_macos = platform.system() == "Darwin"

if is_macos:
    print("ğŸ macOSæ¤œå‡º: M5 Macå‘ã‘è¨­å®šã‚’é©ç”¨ã—ã¾ã™")
    # M5 Macã§ã¯bitsandbytesãŒå‹•ä½œã—ãªã„ãŸã‚ã€é‡å­åŒ–ãªã—ã§ãƒ­ãƒ¼ãƒ‰
    print("  âš ï¸  é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã¯bitsandbytesãŒå¿…è¦ï¼ˆCUDAå°‚ç”¨ï¼‰ã®ãŸã‚ã€é‡å­åŒ–ãªã—ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    model_name = 'openbmb/MiniCPM-Llama3-V-2_5'  # é‡å­åŒ–ãªã—ãƒ¢ãƒ‡ãƒ«
    # quantization_configã‚’Noneã«è¨­å®šã—ã¦é‡å­åŒ–ã‚’ç„¡åŠ¹åŒ–
    quantization_config = None
else:
    print("ğŸªŸ Windowsæ¤œå‡º: é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    model_name = 'openbmb/MiniCPM-Llama3-V-2_5-int4'  # é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«
    # BitsAndBytesConfigã‚’æ˜ç¤ºçš„ã«è¨­å®šã—ã¦é‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name}")
print("   âš ï¸  åˆå›èµ·å‹•æ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
try:
    model = AutoModel.from_pretrained(
        model_name,
        trust_remote_code=True,
        quantization_config=quantization_config,  # é‡å­åŒ–è¨­å®šã‚’æ¸¡ã™
        torch_dtype=torch.float16 if is_macos else None,  # M5 Macã§ã¯float16ã‚’æ¨å¥¨
    )
except Exception as e:
    print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model.eval()

class ImageURL(BaseModel):
    url: str = ""

class Content(BaseModel):
    type: str
    text: str | None = None
    image_url: ImageURL | None = None

class Message(BaseModel):
    role: str
    content: list[Content]

class ChatRequest(BaseModel):
    messages: list[Message]

class Delta(BaseModel):
    role: str = "assistant"
    content: str = ""

class Choice(BaseModel):
    index: int = 0
    finish_reason: str | None = None
    delta: Delta

class ChatResponse(BaseModel):
    id: str = "chatcmpl-00000"
    object: str = "chat.completions.chunk"
    created: int = 0
    model: str = "MiniCPM-Llama3-V-2_5-int4"
    choices: list[Choice]

app = FastAPI()

def base64_to_image(base64_string):
    # Remove the data URI prefix if present
    if "data:image" in base64_string:
        base64_string = base64_string.split(",")[1]

    # Decode the Base64 string into bytes
    image_bytes = base64.b64decode(base64_string)
    return image_bytes

def create_image_from_bytes(image_bytes):
    # Create a BytesIO object to handle the image data
    image_stream = BytesIO(image_bytes)

    # Open the image using Pillow (PIL)
    image = Image.open(image_stream)
    return image

async def chat_generator(chatRequest: ChatRequest):
    image = None
    msgs = []

    for message in chatRequest.messages:
        for content in message.content:
            if content.type == "text":
                msgs.append({'role': message.role, 'content': content.text})
            elif content.type == "image_url" and content.image_url and content.image_url.url:
                image_bytes = base64_to_image(content.image_url.url)
                image = create_image_from_bytes(image_bytes).convert('RGB')

    ## if you want to use streaming, please make sure sampling=True and stream=True
    ## the model.chat will return a generator
    # ç”»åƒãŒãªã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®processorãŒç©ºã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã§ããªã„ãŸã‚ã€
    # é©åˆ‡ãªã‚µã‚¤ã‚ºã®ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ï¼ˆãƒ¢ãƒ‡ãƒ«ã¯448x448ã‚’æœŸå¾…ï¼‰
    if image is None:
        # ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ç”Ÿæˆï¼ˆãƒ¢ãƒ‡ãƒ«ãŒç©ºã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã§ããªã„ãŸã‚ã€448x448ã®é»’ç”»åƒã‚’ä½¿ç”¨ï¼‰
        image = Image.new('RGB', (448, 448), color=(0, 0, 0))
        print("âš ï¸  ç”»åƒãŒãªã„ãŸã‚ã€ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ¢ãƒ¼ãƒ‰ï¼‰")

    res = model.chat(
        image=image,
        msgs=msgs,
        tokenizer=tokenizer,
        sampling=True,
        temperature=0.7,
        stream=True
    )

    generated_text = ""
    index = 0
    for new_text in res:
        generated_text += new_text
        print(new_text, flush=True, end='')
        delta = Delta(role="assistant", content=new_text)
        choice = Choice(index=index, finish_reason=None, delta=delta)
        chatResponse = ChatResponse(choices=[choice])
        index += 1
        yield chatResponse.model_dump_json()
    delta = Delta(role="assistant", content="")
    choice = Choice(index=index, finish_reason="stop", delta=delta)
    chatResponse = ChatResponse(choices=[choice])
    yield chatResponse.model_dump_json()


@app.post("/v1/chat/completions")
def chat_completions(chatRequest: ChatRequest):
    return EventSourceResponse(chat_generator(chatRequest))
