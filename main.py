# main.py
import base64
import json
import platform
import os
import sys
import importlib.metadata
from typing import Union, List
from pydantic import BaseModel
from fastapi import FastAPI, Response
from sse_starlette.sse import EventSourceResponse
import torch
from PIL import Image
from io import BytesIO
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig

# ãƒ€ãƒŸãƒ¼ã®bitsandbytesãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦importlib.metadata.versionã®ãƒã‚§ãƒƒã‚¯ã‚’å›é¿
class DummyBitsAndBytes:
    __version__ = "0.0.0"  # é©å½“ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¨­å®š
sys.modules['bitsandbytes'] = DummyBitsAndBytes()

# M5 Macã®å ´åˆã€é‡å­åŒ–ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
is_macos = platform.system() == "Darwin"

if is_macos:
    print("ğŸ macOSæ¤œå‡º: M5 Macå‘ã‘è¨­å®šã‚’é©ç”¨ã—ã¾ã™")
    # M5 Macã§ã¯bitsandbytesãŒå‹•ä½œã—ãªã„ãŸã‚ã€é‡å­åŒ–ãªã—ã§ãƒ­ãƒ¼ãƒ‰
    print("  âš ï¸  é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã¯bitsandbytesãŒå¿…è¦ï¼ˆCUDAå°‚ç”¨ï¼‰ã®ãŸã‚ã€é‡å­åŒ–ãªã—ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    model_name = 'openbmb/MiniCPM-Llama3-V-2_5'  # é‡å­åŒ–ãªã—ãƒ¢ãƒ‡ãƒ«
    # quantization_configã‚’Noneã«è¨­å®šã—ã¦é‡å­åŒ–ã‚’ç„¡åŠ¹åŒ–
    quantization_config = None
else:
    print("ğŸªŸ Windowsæ¤œå‡º: é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    model_name = 'openbmb/MiniCPM-Llama3-V-2_5-int4'  # é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«
    # BitsAndBytesConfigã‚’æ˜ç¤ºçš„ã«è¨­å®šã—ã¦é‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name}")
print("   âš ï¸  åˆå›èµ·å‹•æ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
try:
    model = AutoModel.from_pretrained(
        model_name,
        trust_remote_code=True,
        quantization_config=quantization_config,  # é‡å­åŒ–è¨­å®šã‚’æ¸¡ã™
        torch_dtype=torch.float16 if is_macos else None,  # M5 Macã§ã¯float16ã‚’æ¨å¥¨
    )
except Exception as e:
    print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model.eval()

class ImageURL(BaseModel):
    url: str = ""

class Content(BaseModel):
    type: str
    text: str | None = None
    image_url: ImageURL | None = None

class Message(BaseModel):
    role: str
    content: list[Content]

class ChatRequest(BaseModel):
    messages: list[Message]
    # OpenAIäº’æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    temperature: float = 0.7
    max_tokens: int = 200
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    stop: list[str] | None = None
    stream: bool = True
    # text-generation-webuiäº’æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    mode: str = "instruct"
    skip_special_tokens: bool = False
    custom_token_bans: str = ""

class Delta(BaseModel):
    role: str = "assistant"
    content: str = ""

class Choice(BaseModel):
    index: int = 0
    finish_reason: str | None = None
    delta: Delta

class ChatResponse(BaseModel):
    id: str = "chatcmpl-00000"
    object: str = "chat.completions.chunk"
    created: int = 0
    model: str = "MiniCPM-Llama3-V-2_5-int4"
    choices: list[Choice]

app = FastAPI()

def base64_to_image(base64_string):
    # Remove the data URI prefix if present
    if "data:image" in base64_string:
        base64_string = base64_string.split(",")[1]

    # Decode the Base64 string into bytes
    image_bytes = base64.b64decode(base64_string)
    return image_bytes

def create_image_from_bytes(image_bytes):
    # Create a BytesIO object to handle the image data
    image_stream = BytesIO(image_bytes)

    # Open the image using Pillow (PIL)
    image = Image.open(image_stream)
    return image

def chat_generator(chatRequest: ChatRequest):
    image = None
    msgs = []

    for message in chatRequest.messages:
        for content in message.content:
            if content.type == "text":
                msgs.append({'role': message.role, 'content': content.text})
            elif content.type == "image_url" and content.image_url and content.image_url.url:
                image_bytes = base64_to_image(content.image_url.url)
                image = create_image_from_bytes(image_bytes).convert('RGB')

    ## if you want to use streaming, please make sure sampling=True and stream=True
    ## the model.chat will return a generator
    # ç”»åƒãŒãªã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®processorãŒç©ºã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã§ããªã„ãŸã‚ã€
    # é©åˆ‡ãªã‚µã‚¤ã‚ºã®ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ï¼ˆãƒ¢ãƒ‡ãƒ«ã¯448x448ã‚’æœŸå¾…ï¼‰
    if image is None:
        # ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ç”Ÿæˆï¼ˆãƒ¢ãƒ‡ãƒ«ãŒç©ºã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã§ããªã„ãŸã‚ã€448x448ã®é»’ç”»åƒã‚’ä½¿ç”¨ï¼‰
        image = Image.new('RGB', (448, 448), color=(0, 0, 0))
        print("âš ï¸  ç”»åƒãŒãªã„ãŸã‚ã€ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ¢ãƒ¼ãƒ‰ï¼‰")

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    temperature = chatRequest.temperature
    max_tokens = chatRequest.max_tokens
    top_p = chatRequest.top_p if chatRequest.top_p < 1.0 else None  # top_p=1.0ã®å ´åˆã¯Noneï¼ˆç„¡åŠ¹åŒ–ï¼‰
    stop_strings = chatRequest.stop if chatRequest.stop else []
    stream = chatRequest.stream

    # model.chat()ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    chat_kwargs = {
        "image": image,
        "msgs": msgs,
        "tokenizer": tokenizer,
        "sampling": True,
        "temperature": temperature,
        "stream": stream,
    }

    # top_pãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¿½åŠ ï¼ˆMiniCPM-VãŒå¯¾å¿œã—ã¦ã„ã‚‹ã‹ã¯ä¸æ˜ã ãŒã€è©¦ã—ã¦ã¿ã‚‹ï¼‰
    if top_p is not None:
        chat_kwargs["top_p"] = top_p

    # max_tokensãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã«åˆ¶é™ã‚’é©ç”¨
    res = model.chat(**chat_kwargs)

    generated_text = ""
    index = 0
    token_count = 0

    for new_text in res:
        # max_tokensåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        if max_tokens > 0 and token_count >= max_tokens:
            break

        # stopæ–‡å­—åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
        generated_text += new_text
        should_stop = False
        for stop_str in stop_strings:
            if stop_str in generated_text:
                # stopæ–‡å­—åˆ—ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®å‰ã¾ã§ã‚’è¿”ã™
                stop_index = generated_text.find(stop_str)
                if stop_index >= 0:
                    generated_text = generated_text[:stop_index]
                    should_stop = True
                    break

        if should_stop:
            break

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¦‚ç®—ï¼ˆç°¡æ˜“çš„ã«æ–‡å­—æ•°ã‹ã‚‰æ¨å®šï¼‰
        token_count += len(new_text.split())

        print(new_text, flush=True, end='')
        delta = Delta(role="assistant", content=new_text)
        choice = Choice(index=index, finish_reason=None, delta=delta)
        chatResponse = ChatResponse(choices=[choice])
        index += 1
        yield chatResponse.model_dump_json()

    # æœ€çµ‚ãƒãƒ£ãƒ³ã‚¯ï¼ˆfinish_reason="stop"ï¼‰
    delta = Delta(role="assistant", content="")
    finish_reason = "stop" if (max_tokens > 0 and token_count >= max_tokens) or should_stop else "stop"
    choice = Choice(index=index, finish_reason=finish_reason, delta=delta)
    chatResponse = ChatResponse(choices=[choice])
    yield chatResponse.model_dump_json()


@app.post("/v1/chat/completions")
def chat_completions(chatRequest: ChatRequest):
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
    if chatRequest.stream:
        return EventSourceResponse(chat_generator(chatRequest))
    else:
        # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆåŒæœŸå¿œç­”ï¼‰
        try:
            # é€šå¸¸ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’åé›†
            result_text = ""
            for chunk_json in chat_generator(chatRequest):
                chunk = json.loads(chunk_json)
                if chunk.get("choices") and len(chunk["choices"]) > 0:
                    delta = chunk["choices"][0].get("delta", {})
                    content = delta.get("content", "")
                    if content:
                        result_text += content

            # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
            from datetime import datetime
            response = {
                "id": "chatcmpl-00000",
                "object": "chat.completion",
                "created": int(datetime.now().timestamp()),
                "model": "MiniCPM-Llama3-V-2_5-int4",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result_text
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 0,  # ç°¡æ˜“å®Ÿè£…ã®ãŸã‚0
                    "completion_tokens": 0,  # ç°¡æ˜“å®Ÿè£…ã®ãŸã‚0
                    "total_tokens": 0  # ç°¡æ˜“å®Ÿè£…ã®ãŸã‚0
                }
            }
            return response
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}, 500
